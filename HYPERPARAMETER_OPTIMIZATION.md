# Hyperparameter Optimization - Guide Complet

## üéØ Objectif

Trouver les **meilleurs mod√®les pr√©dictifs** pour l'√¢ge √©pig√©n√©tique en testant exhaustivement toutes les m√©thodes ML populaires avec optimisation bay√©sienne des hyperparam√®tres.

**Approche Senior Data Scientist** :
- ‚úÖ Optuna (optimisation bay√©sienne TPE + √©lagage m√©dian)
- ‚úÖ 10 algorithmes test√©s (Ridge, Lasso, ElasticNet, SVR, RF, GBM, XGBoost, LightGBM, CatBoost, MLP)
- ‚úÖ 50-150 trials par mod√®le selon complexit√©
- ‚úÖ Cross-validation 5-fold pour robustesse
- ‚úÖ Gestion m√©moire optimis√©e (PCA ou s√©lection features)
- ‚úÖ Sauvegarde progressive (SQLite)
- ‚úÖ Suivi temps r√©el
- ‚úÖ Rapport d√©taill√© (CSV + logs)

---

## üì¶ Installation

### D√©pendances Requises

```bash
# Installer toutes les biblioth√®ques d'optimisation
pip install optuna lightgbm catboost

# Ou r√©installer depuis requirements.txt
pip install -r requirements.txt
```

### V√©rifier Installation

```bash
python -c "import optuna, lightgbm, catboost; print('OK')"
```

---

## üöÄ Utilisation

### Option 1: Mode Standard (5000 features s√©lectionn√©es)

```bash
python scripts/hyperparameter_optimization.py \
    --top-k-features 5000 \
    --max-hours 8.0 \
    --test-size 0.2
```

**Temps estim√©** : 6-8 heures
**M√©moire** : ~4-8 GB RAM

### Option 2: Mode PCA (r√©duction dimensionnalit√©)

```bash
python scripts/hyperparameter_optimization.py \
    --use-pca \
    --pca-components 400 \
    --max-hours 6.0 \
    --test-size 0.2
```

**Temps estim√©** : 4-6 heures
**M√©moire** : ~2-4 GB RAM
**Avantage** : Plus rapide, moins de m√©moire

### Option 3: Optimiser Mod√®les Sp√©cifiques

```bash
# Seulement Ridge, Lasso, ElasticNet
python scripts/hyperparameter_optimization.py \
    --models Ridge Lasso ElasticNet \
    --max-hours 2.0

# Seulement les boosting methods
python scripts/hyperparameter_optimization.py \
    --models XGBoost LightGBM CatBoost \
    --max-hours 4.0
```

### Option 4: Mode Rapide (Test)

```bash
python scripts/hyperparameter_optimization.py \
    --top-k-features 1000 \
    --max-hours 1.0 \
    --models Ridge XGBoost
```

**Temps estim√©** : ~1 heure
**Usage** : Test rapide pour v√©rifier fonctionnement

---

## üìä Mod√®les Optimis√©s

### 1. **Mod√®les Lin√©aires** (rapides, interpr√©tables)

#### Ridge Regression
- **Hyperparam√®tres** : alpha (1e-3 √† 1e5), solver
- **Trials** : 100
- **Temps** : ~10-15 min
- **Avantages** : Robuste, stable, bon avec high-dimensional data

#### Lasso Regression
- **Hyperparam√®tres** : alpha (1e-5 √† 10), max_iter, selection
- **Trials** : 100
- **Temps** : ~10-15 min
- **Avantages** : Feature selection automatique (sparse)

#### ElasticNet
- **Hyperparam√®tres** : alpha, l1_ratio, max_iter, selection
- **Trials** : 150
- **Temps** : ~15-20 min
- **Avantages** : Meilleur des deux mondes (L1 + L2)

---

### 2. **Support Vector Machine**

#### SVR (Support Vector Regressor)
- **Hyperparam√®tres** : kernel (linear/rbf/poly), C, epsilon, gamma, degree
- **Trials** : 100
- **Temps** : ~30-45 min
- **Avantages** : Capture non-lin√©arit√©s complexes
- **Inconv√©nient** : Lent sur grands datasets

---

### 3. **Mod√®les Ensembles Classiques**

#### Random Forest
- **Hyperparam√®tres** : n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap
- **Trials** : 80
- **Temps** : ~20-30 min
- **Avantages** : Robuste aux outliers, peu d'overfitting

#### Gradient Boosting (scikit-learn)
- **Hyperparam√®tres** : n_estimators, learning_rate, max_depth, min_samples_split, subsample, max_features
- **Trials** : 80
- **Temps** : ~30-45 min
- **Avantages** : G√©n√©ralement excellentes performances

---

### 4. **Gradient Boosting Moderne** (SOTA)

#### XGBoost
- **Hyperparam√®tres** : n_estimators, learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda, gamma
- **Trials** : 100
- **Temps** : ~30-45 min
- **Avantages** : Tr√®s performant, r√©gularisation forte, GPU support

#### LightGBM
- **Hyperparam√®tres** : n_estimators, learning_rate, num_leaves, max_depth, min_child_samples, subsample, colsample_bytree, reg_alpha, reg_lambda
- **Trials** : 100
- **Temps** : ~20-30 min
- **Avantages** : TR√àS RAPIDE, √©conome en m√©moire, excellent pour high-dimensional

#### CatBoost
- **Hyperparam√®tres** : iterations, learning_rate, depth, l2_leaf_reg, border_count
- **Trials** : 80
- **Temps** : ~40-60 min
- **Avantages** : Bon avec donn√©es cat√©gorielles, robuste, peu de tuning n√©cessaire

---

### 5. **R√©seaux de Neurones**

#### MLP (Multi-Layer Perceptron)
- **Hyperparam√®tres** : n_layers (1-4), n_units_per_layer (32-512), activation, alpha, learning_rate, learning_rate_init
- **Trials** : 100
- **Temps** : ~45-60 min
- **Avantages** : Capture relations tr√®s non-lin√©aires
- **Inconv√©nient** : Peut overfitter, moins interpr√©table

---

## üìà Espace de Recherche Hyperparam√®tres

### Ridge
```python
{
    'alpha': [1e-3, 1e5] (log-scale),  # R√©gularisation L2
    'solver': ['auto', 'svd', 'cholesky', 'lsqr']
}
```

### XGBoost (exemple d√©taill√©)
```python
{
    'n_estimators': [50, 500],           # Nombre d'arbres
    'learning_rate': [1e-3, 0.3] (log),  # Taux d'apprentissage
    'max_depth': [2, 12],                # Profondeur arbres
    'min_child_weight': [1, 10],         # Min samples leaf
    'subsample': [0.5, 1.0],             # Row sampling
    'colsample_bytree': [0.5, 1.0],      # Column sampling
    'reg_alpha': [1e-3, 100] (log),      # L1 regularization
    'reg_lambda': [1e-3, 100] (log),     # L2 regularization
    'gamma': [1e-3, 10] (log)            # Min split loss
}
```

### LightGBM (optimis√© vitesse)
```python
{
    'n_estimators': [50, 500],
    'learning_rate': [1e-3, 0.3] (log),
    'num_leaves': [10, 200],             # Complexit√© arbre
    'max_depth': [2, 12],
    'min_child_samples': [5, 50],
    'subsample': [0.5, 1.0],
    'colsample_bytree': [0.5, 1.0],
    'reg_alpha': [1e-3, 100] (log),
    'reg_lambda': [1e-3, 100] (log)
}
```

---

## üîç Strat√©gie d'Optimisation

### Optuna TPE Sampler
- **Tree-structured Parzen Estimator** (TPE)
- Optimisation bay√©sienne intelligente
- Apprend des trials pr√©c√©dents
- Concentre la recherche sur zones prometteuses

### Pruning
- **MedianPruner** avec `n_startup_trials=10`
- Arr√™te les trials non prometteurs t√¥t
- √âconomise du temps de calcul
- Bas√© sur m√©diane des CV scores

### Cross-Validation
- **5-fold CV** par d√©faut
- Robuste, √©vite overfitting
- MAE moyen utilis√© comme m√©trique

---

## üìÅ R√©sultats G√©n√©r√©s

### Structure des Fichiers

```
results/optimization/
‚îú‚îÄ‚îÄ optuna_study.db                          # Base SQLite (tous trials)
‚îú‚îÄ‚îÄ optimization_results_YYYYMMDD_HHMMSS.csv # Tableau comparatif
‚îú‚îÄ‚îÄ best_hyperparameters_YYYYMMDD_HHMMSS.csv # Tous hyperparam√®tres
‚îú‚îÄ‚îÄ scaler.joblib                            # StandardScaler fitted
‚îú‚îÄ‚îÄ imputer.joblib                           # KNN Imputer fitted
‚îú‚îÄ‚îÄ pca_transformer.joblib                   # PCA (si --use-pca)
‚îú‚îÄ‚îÄ best_ridge.joblib                        # Meilleur mod√®le Ridge
‚îú‚îÄ‚îÄ best_xgboost.joblib                      # Meilleur mod√®le XGBoost
‚îú‚îÄ‚îÄ best_lightgbm.joblib                     # Meilleur mod√®le LightGBM
‚îî‚îÄ‚îÄ ...
```

### Format CSV R√©sultats

```csv
Rank,Model,MAE_Train,MAE_Test,MAD_Test,R2_Train,R2_Test,Overfitting_Ratio,CV_MAE,N_Params,N_Trials,Optimization_Time_Min
1,LightGBM,2.145,3.234,2.876,0.9821,0.9678,1.51,3.156,350,100,25.4
2,XGBoost,2.223,3.298,2.934,0.9803,0.9665,1.48,3.201,400,100,32.1
3,Ridge,2.567,3.412,3.012,0.9745,0.9634,1.33,3.389,5002,100,12.3
...
```

### Colonnes Expliqu√©es

- **Rank** : Classement (1 = meilleur)
- **Model** : Nom du mod√®le
- **MAE_Train** : MAE sur ensemble d'entra√Ænement
- **MAE_Test** : MAE sur ensemble de test (M√âTRIQUE CLEF)
- **MAD_Test** : Median Absolute Deviation (robuste aux outliers)
- **R2_Train** : R¬≤ sur train
- **R2_Test** : R¬≤ sur test
- **Overfitting_Ratio** : MAE_Test / MAE_Train (< 2.0 excellent)
- **CV_MAE** : MAE cross-validation (moyenne)
- **N_Params** : Nombre de param√®tres du mod√®le
- **N_Trials** : Nombre de trials Optuna ex√©cut√©s
- **Optimization_Time_Min** : Temps d'optimisation (minutes)

---

## ‚è±Ô∏è Temps d'Ex√©cution Estim√©s

### Configuration Minimale (1000 features, 2h max)
- Ridge: ~5 min
- Lasso: ~5 min
- ElasticNet: ~7 min
- XGBoost: ~15 min
- LightGBM: ~10 min
- **Total**: ~1h

### Configuration Standard (5000 features, 8h max)
- Ridge: ~12 min
- Lasso: ~12 min
- ElasticNet: ~18 min
- SVR: ~40 min
- RandomForest: ~25 min
- GradientBoosting: ~35 min
- XGBoost: ~35 min
- LightGBM: ~25 min
- CatBoost: ~50 min
- MLP: ~55 min
- **Total**: ~6-7h

### Configuration PCA (400 components, 6h max)
- Tous mod√®les ~30% plus rapides
- **Total**: ~4-5h

---

## üí° Conseils d'Utilisation

### Pour Minimiser le Temps

1. **Utiliser PCA** : R√©duit dimensionnalit√© drastiquement
   ```bash
   --use-pca --pca-components 200
   ```

2. **S√©lectionner moins de features**
   ```bash
   --top-k-features 2000
   ```

3. **Optimiser seulement top mod√®les**
   ```bash
   --models XGBoost LightGBM Ridge
   ```

4. **R√©duire max_hours**
   ```bash
   --max-hours 4.0
   ```

### Pour Maximiser la Performance

1. **Plus de features** (si m√©moire suffisante)
   ```bash
   --top-k-features 10000
   ```

2. **Tous les mod√®les** (laisser tourner)
   ```bash
   --max-hours 8.0
   ```

3. **Test size plus petit** (plus de donn√©es train)
   ```bash
   --test-size 0.15
   ```

### Gestion M√©moire

**Si RAM < 8 GB** :
- Utiliser `--use-pca --pca-components 200`
- Ou `--top-k-features 2000`

**Si RAM >= 16 GB** :
- Peut utiliser `--top-k-features 10000` sans probl√®me

**Si RAM >= 32 GB** :
- Peut charger toutes les features avec PCA

---

## üî¨ Analyse des R√©sultats

### 1. Charger les R√©sultats

```python
import pandas as pd
import joblib

# Charger tableau comparatif
results = pd.read_csv('results/optimization/optimization_results_*.csv')
print(results.sort_values('MAE_Test').head(5))

# Charger meilleur mod√®le
best_model = joblib.load('results/optimization/best_lightgbm.joblib')
scaler = joblib.load('results/optimization/scaler.joblib')
```

### 2. Pr√©dire sur Nouvelles Donn√©es

```python
import numpy as np

# Charger transformers
scaler = joblib.load('results/optimization/scaler.joblib')
imputer = joblib.load('results/optimization/imputer.joblib')

# Optionnel: PCA si utilis√©
pca = joblib.load('results/optimization/pca_transformer.joblib')

# Pr√©parer donn√©es
X_new = ...  # Vos nouvelles donn√©es
X_new = imputer.transform(X_new)
if pca:
    X_new = pca.transform(X_new)
X_new = scaler.transform(X_new)

# Pr√©dire
predictions = best_model.predict(X_new)
print(f"Ages pr√©dits: {predictions}")
```

### 3. Explorer Base Optuna

```python
import optuna

# Charger study
storage = 'sqlite:///results/optimization/optuna_study.db'
study_name = 'XGBoost_20260128_120000'  # Adapter
study = optuna.load_study(study_name=study_name, storage=storage)

# Best trial
print(f"Best MAE: {study.best_value:.3f}")
print(f"Best params: {study.best_params}")

# Historique
df = study.trials_dataframe()
print(df[['number', 'value', 'state']].head(10))

# Visualisations
from optuna.visualization import plot_optimization_history
fig = plot_optimization_history(study)
fig.show()
```

---

## üéØ Crit√®res de Succ√®s

### Excellent Mod√®le
- ‚úÖ MAE Test < 3.5 ans
- ‚úÖ R¬≤ Test > 0.95
- ‚úÖ Overfitting Ratio < 2.0x
- ‚úÖ MAD Test ‚âà MAE Test (pas d'outliers)

### Bon Mod√®le
- ‚úÖ MAE Test < 4.5 ans
- ‚úÖ R¬≤ Test > 0.92
- ‚úÖ Overfitting Ratio < 3.0x

### Acceptable
- ‚ö†Ô∏è MAE Test < 6.0 ans
- ‚ö†Ô∏è R¬≤ Test > 0.85
- ‚ö†Ô∏è Overfitting Ratio < 5.0x

### Probl√©matique
- ‚ùå MAE Test > 6.0 ans
- ‚ùå Overfitting Ratio > 10x

---

## üêõ Troubleshooting

### Erreur: Out of Memory

**Solution** :
```bash
# Utiliser PCA
python scripts/hyperparameter_optimization.py --use-pca --pca-components 200

# Ou r√©duire features
python scripts/hyperparameter_optimization.py --top-k-features 1000
```

### Erreur: Import Error (LightGBM/CatBoost)

**Solution** :
```bash
pip install lightgbm catboost

# Ou optimiser sans ces mod√®les
python scripts/hyperparameter_optimization.py --models Ridge Lasso ElasticNet XGBoost
```

### Processus Trop Lent

**Solution** :
```bash
# R√©duire nombre de trials (modifier dans le script)
# Ou limiter mod√®les
python scripts/hyperparameter_optimization.py --models XGBoost LightGBM --max-hours 2
```

### Interrompre et Reprendre

**L'optimisation peut √™tre interrompue** (Ctrl+C) √† tout moment :
- ‚úÖ R√©sultats d√©j√† compl√©t√©s sont sauvegard√©s
- ‚úÖ Base Optuna pr√©serv√©e
- ‚ö†Ô∏è Mod√®le en cours perdu (mais pas grave)

**Relancer** reprend avec nouveaux mod√®les (ne reprend PAS trials pr√©c√©dents car nouveaux study names).

---

## üìö R√©f√©rences Scientifiques

### Optimisation Bay√©sienne
- Akiba et al. (2019). "Optuna: A Next-generation Hyperparameter Optimization Framework." *KDD 2019*.

### Gradient Boosting
- Chen & Guestrin (2016). "XGBoost: A Scalable Tree Boosting System." *KDD 2016*.
- Ke et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." *NeurIPS 2017*.
- Prokhorenkova et al. (2018). "CatBoost: unbiased boosting with categorical features." *NeurIPS 2018*.

---

## ‚úÖ Checklist Utilisation

- [ ] Installer d√©pendances (`pip install -r requirements.txt`)
- [ ] V√©rifier m√©moire disponible (au moins 4 GB RAM recommand√©)
- [ ] Choisir strat√©gie: PCA ou s√©lection features
- [ ] Lancer optimisation avec param√®tres adapt√©s
- [ ] Surveiller logs en temps r√©el
- [ ] Attendre fin (ou interrompre si satisfait)
- [ ] Analyser `optimization_results_*.csv`
- [ ] Charger meilleur mod√®le et tester
- [ ] Utiliser meilleur mod√®le pour pr√©dictions futures

---

**Date** : 2026-01-28
**Auteur** : Claude Opus 4.5
**Version** : 1.0
**Status** : ‚úÖ Pr√™t pour production
