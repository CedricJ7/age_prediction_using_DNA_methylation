data:
  data_dir: "Data"
  top_k_features: 5000  # Reduced from 10000 to prevent overfitting
  chunk_size: 2000
  test_size: 0.2
  use_pca: false
  pca_components: 400
  missing_rate_threshold: 0.05

models:
  ridge_alpha: 5000.0  # Increased from 100 to reduce overfitting (39x â†’ target <5x)
  elasticnet_alpha: 0.1
  elasticnet_l1_ratio: 0.5
  lasso_alpha: 0.1
  xgboost_n_estimators: 200  # Reduced from 400 to prevent overtraining
  xgboost_learning_rate: 0.05
  xgboost_max_depth: 4  # Reduced from 6 to limit tree complexity
  xgboost_reg_alpha: 10.0  # Increased from 1.0 for stronger L1 regularization
  xgboost_reg_lambda: 50.0  # Increased from 10.0 for stronger L2 regularization
  xgboost_early_stopping_rounds: 20  # NEW: Stop training when validation doesn't improve
  rf_n_estimators: 300
  rf_max_depth: 10  # Reduced from 20 to prevent overfitting
  mlp_hidden_layers: [128, 64, 32]
  mlp_alpha: 0.001
  # DeepMAge parameters (PyTorch-based deep learning)
  deepmage_hidden_size: 512  # Number of neurons in hidden layer
  deepmage_dropout: 0.3  # Dropout probability for regularization
  deepmage_learning_rate: 0.001  # Adam optimizer learning rate
  deepmage_batch_size: 32  # Batch size for training
  deepmage_epochs: 100  # Maximum number of epochs
  deepmage_early_stopping_patience: 10  # Early stopping patience
  deepmage_random_state: 42  # Random seed

optimization:
  cv_folds: 10
  n_iter: 50
  random_state: 42
  scoring: "neg_mean_absolute_error"
  use_optuna: false
  n_jobs: -1

output_dir: "results"
