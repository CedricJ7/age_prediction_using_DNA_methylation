# DeepMAge - Deep Learning pour la Pr√©diction d'√Çge √âpig√©n√©tique

## üìö Contexte Scientifique

**DeepMAge** est un mod√®le d'apprentissage profond inspir√© de l'article de Galkin et al. (2021) qui utilise des r√©seaux de neurones profonds pour pr√©dire l'√¢ge biologique √† partir de profils de m√©thylation de l'ADN.

### R√©f√©rences Cl√©s

- **Galkin, F., et al. (2021).** "DeepMAge: A methylation aging clock developed with deep learning." *Aging and Disease*, 12(5), 1252-1262.
- Approche moderne utilisant PyTorch pour capturer les relations non-lin√©aires complexes dans les donn√©es de m√©thylation

---

## üèóÔ∏è Architecture du Mod√®le

### Structure du R√©seau

```
Input (5000 CpG sites)
    ‚Üì
Linear(5000 ‚Üí 512)
    ‚Üì
BatchNorm1d(512)
    ‚Üì
ReLU
    ‚Üì
Dropout(p=0.3)
    ‚Üì
Linear(512 ‚Üí 1)
    ‚Üì
Output (predicted age)
```

### Composants

1. **Couche d'entr√©e** : 5000 sites CpG (features s√©lectionn√©s)
2. **Couche cach√©e** : 512 neurones avec activation ReLU
3. **Batch Normalization** : Stabilise l'entra√Ænement et acc√©l√®re la convergence
4. **Dropout (30%)** : R√©gularisation pour pr√©venir l'overfitting
5. **Couche de sortie** : 1 neurone (√¢ge pr√©dit en ann√©es)

---

## ‚öôÔ∏è Hyperparam√®tres

### Configuration par D√©faut (`config/model_config.yaml`)

```yaml
models:
  deepmage_hidden_size: 512             # Nombre de neurones dans la couche cach√©e
  deepmage_dropout: 0.3                 # Probabilit√© de dropout
  deepmage_learning_rate: 0.001         # Learning rate pour Adam optimizer
  deepmage_batch_size: 32               # Taille des batchs
  deepmage_epochs: 100                  # Nombre maximum d'√©poques
  deepmage_early_stopping_patience: 10  # Patience pour early stopping
  deepmage_random_state: 42             # Seed pour reproductibilit√©
```

### D√©tails des Param√®tres

- **hidden_size (512)** : Suffisamment large pour capturer les interactions complexes entre CpG sites
- **dropout (0.3)** : R√©gularisation mod√©r√©e pour √©viter l'overfitting sur petit dataset
- **learning_rate (0.001)** : Taux d'apprentissage standard pour Adam
- **batch_size (32)** : Compromis entre stabilit√© et vitesse d'entra√Ænement
- **early_stopping_patience (10)** : Arr√™te si validation ne s'am√©liore pas pendant 10 √©poques

---

## üîß Impl√©mentation Technique

### 1. Fichier Principal

**`src/models/deep_learning.py`** contient :
- `DeepMAge` : Classe PyTorch `nn.Module` d√©finissant l'architecture
- `DeepMAgeRegressor` : Wrapper scikit-learn compatible pour int√©gration facile
- `create_deepmage_model()` : Factory function pour cr√©er le mod√®le

### 2. Caract√©ristiques Cl√©s

#### Initialisation des Poids
```python
# He initialization pour ReLU
nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
```

#### Early Stopping
```python
# Arr√™te l'entra√Ænement si validation ne s'am√©liore pas
if epoch_val_loss < best_val_loss:
    best_val_loss = epoch_val_loss
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= early_stopping_patience:
        break  # Stop training
```

#### Standardisation des Features
```python
# Standardise les features avant entra√Ænement (Œº=0, œÉ=1)
self.scaler_ = StandardScaler()
X_scaled = self.scaler_.fit_transform(X)
```

### 3. D√©tection Automatique GPU

Le mod√®le utilise automatiquement le GPU si disponible :
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

**Pour Ubuntu 24.04** :
```bash
# V√©rifier disponibilit√© CUDA
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# Si GPU NVIDIA disponible, installer PyTorch avec CUDA :
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

## üöÄ Utilisation

### Entra√Ænement

```bash
# Le mod√®le DeepMAge est automatiquement inclus dans le pipeline
python scripts/train.py --config config/model_config.yaml

# Logs attendus :
# Training DeepMAge...
# Using validation set: train=272, val=48
# Epoch 10/100 - Train Loss: 45.2341 - Val Loss: 52.1234
# ...
# Early stopping at epoch 45
# Best epoch: 35 with val loss: 48.5678
```

### Pr√©diction

```python
import joblib
import numpy as np

# Charger le mod√®le entra√Æn√©
model = joblib.load("results/models/deepmage.joblib")

# Pr√©dire sur nouvelles donn√©es
X_new = np.array([...])  # Shape: (n_samples, 5000)
ages_predicted = model.predict(X_new)
```

---

## üìä Avantages de DeepMAge

### 1. Capture des Relations Non-Lin√©aires
- Les r√©seaux de neurones peuvent apprendre des interactions complexes entre sites CpG
- Contrairement aux mod√®les lin√©aires (Ridge, Lasso), pas besoin de sp√©cifier manuellement les interactions

### 2. Regularisation Int√©gr√©e
- **Dropout** : Emp√™che le mod√®le de trop d√©pendre de certains neurones
- **Batch Normalization** : R√©duit l'internal covariate shift
- **Early Stopping** : Arr√™te avant overfitting

### 3. Scalabilit√©
- Peut g√©rer des milliers de features efficacement
- Utilise GPU si disponible pour acc√©l√©ration

### 4. Performance Attendue
D'apr√®s la litt√©rature (Galkin 2021) :
- **MAE** : ~3-4 ans sur donn√©es de validation
- **R¬≤** : ~0.95-0.97
- **Meilleure performance** sur jeunes adultes (20-40 ans)

---

## üî¨ Comparaison avec Autres Approches

| Mod√®le | Type | Complexit√© | Interpr√©tabilit√© | Performance Attendue |
|--------|------|------------|------------------|----------------------|
| **Horvath (2013)** | Elastic Net | Lin√©aire | ‚úÖ Haute (coefficients) | MAE ~4 ans |
| **Hannum (2013)** | WGCNA + Linear | Lin√©aire | ‚úÖ Haute | MAE ~4 ans |
| **PhenoAge (2018)** | Cox Regression | Semi-lin√©aire | ‚úÖ Moyenne | Pr√©dit mortalit√© |
| **DeepMAge (2021)** | Deep Neural Net | ‚ö†Ô∏è Non-lin√©aire | ‚ùå Faible (black box) | MAE ~3 ans |
| **Notre Ridge** | Ridge Regression | Lin√©aire | ‚úÖ Haute | MAE ~3.4 ans |
| **Notre DeepMAge** | PyTorch DNN | ‚ö†Ô∏è Non-lin√©aire | ‚ùå Faible | MAE ~? ans (√† tester) |

---

## ‚ö†Ô∏è Limitations et Consid√©rations

### 1. Overfitting Risk
- **Dataset petit** : 400 samples seulement
- **Solution** : Early stopping + Dropout + Batch Normalization
- **Surveiller** : Train loss vs Validation loss

### 2. Interpr√©tabilit√©
- Mod√®le "bo√Æte noire"
- Difficile d'identifier quels sites CpG sont importants
- **Alternative pour interpr√©tation** : Utiliser SHAP ou Integrated Gradients

### 3. Temps d'Entra√Ænement
- Plus lent que Ridge/Lasso
- **CPU** : ~2-5 minutes
- **GPU** : ~30-60 secondes

### 4. Reproductibilit√©
- N√©cessite seed fixe pour r√©sultats identiques
- Variance due √† initialisation al√©atoire des poids

---

## üìà Tuning des Hyperparam√®tres

### Sc√©narios d'Ajustement

#### Si Overfitting (Train Loss << Val Loss) :
```yaml
deepmage_dropout: 0.5                # Augmenter dropout
deepmage_early_stopping_patience: 5  # R√©duire patience
deepmage_learning_rate: 0.0005       # R√©duire learning rate
```

#### Si Underfitting (Train Loss et Val Loss hauts) :
```yaml
deepmage_hidden_size: 1024           # Plus de neurones
deepmage_epochs: 200                 # Plus d'√©poques
deepmage_learning_rate: 0.01         # Augmenter learning rate
```

#### Si Instabilit√© :
```yaml
deepmage_batch_size: 16              # R√©duire batch size
deepmage_learning_rate: 0.0001       # R√©duire learning rate
```

---

## üß™ Validation

### V√©rifier le Mod√®le Entra√Æn√©

```python
import joblib
import pandas as pd

# Charger mod√®le et r√©sultats
model = joblib.load("results/models/deepmage.joblib")
metrics = pd.read_csv("results/metrics.csv")

# Afficher performance DeepMAge
deepmage_metrics = metrics[metrics["model"] == "DeepMAge"]
print(deepmage_metrics[["mae", "r2", "overfitting_ratio"]])

# V√©rifier historique d'entra√Ænement
if hasattr(model, 'training_history_'):
    history = pd.DataFrame(model.training_history_)
    print(history.tail(10))
```

### Crit√®res de Succ√®s

‚úÖ **Bon mod√®le** si :
- MAE test < 5.0 ans
- R¬≤ > 0.90
- Overfitting ratio < 5.0x
- Early stopping avant √©poque 100 (indique bon r√©glage)

‚ö†Ô∏è **Probl√®me** si :
- MAE test > 8.0 ans
- R¬≤ < 0.80
- Overfitting ratio > 10.0x
- Convergence apr√®s 100 √©poques (augmenter epochs)

---

## üîÑ Workflow Complet

### 1. Installation
```bash
# Installer PyTorch
pip install torch>=2.0.0

# Ou avec CUDA pour GPU
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. Entra√Ænement
```bash
python scripts/train.py --config config/model_config.yaml
```

### 3. Visualisation
```bash
python app.py
# Ouvrir http://localhost:8050
# S√©lectionner "DeepMAge" dans le dropdown
```

### 4. Export Rapport
- Cliquer sur "Export Report"
- Le rapport PDF incluera automatiquement DeepMAge dans les comparaisons

---

## üìö R√©f√©rences Compl√®tes

1. **Galkin, F., Mamoshina, P., Aliper, A., Putin, E., Moskalev, V., Gladyshev, V. N., & Zhavoronkov, A. (2021).** DeepMAge: A methylation aging clock developed with deep learning. *Aging and Disease*, 12(5), 1252-1262.

2. **Horvath, S. (2013).** DNA methylation age of human tissues and cell types. *Genome Biology*, 14(10), R115.

3. **Hannum, G., Guinney, J., Zhao, L., et al. (2013).** Genome-wide methylation profiles reveal quantitative views of human aging rates. *Molecular Cell*, 49(2), 359-367.

4. **Levine, M. E., Lu, A. T., Quach, A., et al. (2018).** An epigenetic biomarker of aging for lifespan and healthspan. *Aging*, 10(4), 573-591.

5. **Goodfellow, I., Bengio, Y., & Courville, A. (2016).** Deep Learning. MIT Press. *(Chapitre 7: Regularization)*

---

## üí° Perspectives Futures

### Am√©liorations Possibles

1. **Architecture Plus Profonde**
   ```python
   Input ‚Üí 512 ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout
         ‚Üí 256 ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout
         ‚Üí 128 ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout
         ‚Üí 1
   ```

2. **Attention Mechanism**
   - Identifier automatiquement les sites CpG importants
   - Am√©liore interpr√©tabilit√©

3. **Residual Connections**
   - Permet entra√Ænement de r√©seaux plus profonds
   - √âvite vanishing gradients

4. **Ensemble avec Mod√®les Lin√©aires**
   - Combiner DeepMAge + Ridge pour meilleure robustesse

---

**Date** : 2026-01-28
**Auteur** : Claude Opus 4.5
**Version** : 1.0
**Status** : ‚úÖ Impl√©ment√© et pr√™t pour entra√Ænement
