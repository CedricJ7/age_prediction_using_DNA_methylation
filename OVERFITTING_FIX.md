# ğŸ”§ Correction de l'Overfitting - Analyse et Solutions

## ğŸ“Š ProblÃ¨me DÃ©tectÃ©

Lors de l'entraÃ®nement prÃ©cÃ©dent, **overfitting SÃ‰VÃˆRE** dÃ©tectÃ© sur plusieurs modÃ¨les :

| ModÃ¨le | MAE Test | RÂ² | Overfitting Ratio | Statut |
|--------|----------|-----|-------------------|--------|
| **Ridge** | 3.415 | 0.961 | **39.3x** âš ï¸ | CRITIQUE |
| **XGBoost** | 4.465 | 0.938 | **63.1x** âš ï¸âš ï¸ | CRITIQUE |
| ElasticNet | 3.594 | 0.955 | 8.2x | ModÃ©rÃ© |
| Lasso | 3.814 | 0.950 | 5.1x | Acceptable |
| RandomForest | 5.619 | 0.903 | 2.6x | âœ… Bon |
| AltumAge | 15.305 | 0.290 | 1.6x | âœ… Bon (mais mauvaise perf) |

### ğŸ¯ Objectif
RÃ©duire l'overfitting Ã  **< 5x** pour tous les modÃ¨les.

---

## âœ… Solutions ImplÃ©mentÃ©es

### 1ï¸âƒ£ **RÃ©duction du nombre de features**
**Avant :** `top_k_features: 10000`
**AprÃ¨s :** `top_k_features: 5000` âœ…

**Justification :**
- 10,000 features pour 320 samples d'entraÃ®nement = ratio 31:1
- 5,000 features = ratio 16:1 (meilleur)
- Moins de features = moins de risque de surapprendre les patterns spurieux

---

### 2ï¸âƒ£ **Augmentation de la rÃ©gularisation Ridge**
**Avant :** `ridge_alpha: 100.0`
**AprÃ¨s :** `ridge_alpha: 5000.0` âœ… (50x augmentation)

**Justification :**
- Ridge avait overfitting 39.3x (CRITIQUE)
- RÃ©gularisation L2 beaucoup plus forte
- PÃ©nalise davantage les coefficients Ã©levÃ©s
- Cible : ramener overfitting < 5x

**Formule Ridge :**
```
Loss = MSE + alpha * ||Î²||Â²
```
Plus alpha est grand, plus les coefficients sont contraints.

---

### 3ï¸âƒ£ **Renforcement de la rÃ©gularisation XGBoost**

**Avant :**
```yaml
xgboost_reg_alpha: 1.0    # L1
xgboost_reg_lambda: 10.0  # L2
```

**AprÃ¨s :**
```yaml
xgboost_reg_alpha: 10.0   # L1 (10x augmentation)
xgboost_reg_lambda: 50.0  # L2 (5x augmentation)
```

**Justification :**
- XGBoost avait overfitting 63.1x (CRITIQUE++)
- RÃ©gularisation L1 (sparsity) + L2 (shrinkage)
- PÃ©nalise les arbres trop complexes

---

### 4ï¸âƒ£ **RÃ©duction de la complexitÃ© XGBoost**

**Avant :**
```yaml
xgboost_n_estimators: 400
xgboost_max_depth: 6
```

**AprÃ¨s :**
```yaml
xgboost_n_estimators: 200  # MoitiÃ© moins d'arbres
xgboost_max_depth: 4       # Arbres moins profonds
```

**Justification :**
- Moins d'arbres = moins de capacitÃ© Ã  mÃ©moriser
- max_depth 4 au lieu de 6 = arbres plus simples
- RÃ©duit le risque de surapprendre les dÃ©tails

---

### 5ï¸âƒ£ **Early Stopping pour XGBoost** ğŸ†•

**Nouveau paramÃ¨tre :**
```yaml
xgboost_early_stopping_rounds: 20
```

**ImplÃ©mentation :**
```python
# Split train en train/validation (85/15)
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15)

# Fit avec eval_set
model.fit(
    X_tr, y_tr,
    eval_set=[(X_val, y_val)],
    verbose=False
)
```

**Justification :**
- ArrÃªte l'entraÃ®nement quand la validation ne s'amÃ©liore plus
- Ã‰vite de surentraÃ®ner au-delÃ  du point optimal
- DÃ©tecte automatiquement le nombre d'itÃ©rations optimal

**Fonctionnement :**
Si la MAE sur validation ne s'amÃ©liore pas pendant 20 itÃ©rations consÃ©cutives â†’ STOP.

---

### 6ï¸âƒ£ **RÃ©duction de la complexitÃ© Random Forest**

**Avant :** `rf_max_depth: 20`
**AprÃ¨s :** `rf_max_depth: 10` âœ…

**Justification :**
- Random Forest avait overfitting 2.6x (dÃ©jÃ  acceptable)
- RÃ©duction prÃ©ventive pour sÃ©curitÃ©
- Arbres moins profonds = moins de mÃ©morisation

---

## ğŸ“‹ RÃ©sumÃ© des Changements

### Fichiers modifiÃ©s :

#### 1. `config/model_config.yaml`
```diff
data:
- top_k_features: 10000
+ top_k_features: 5000

models:
- ridge_alpha: 100.0
+ ridge_alpha: 5000.0

- xgboost_n_estimators: 400
+ xgboost_n_estimators: 200

- xgboost_max_depth: 6
+ xgboost_max_depth: 4

- xgboost_reg_alpha: 1.0
+ xgboost_reg_alpha: 10.0

- xgboost_reg_lambda: 10.0
+ xgboost_reg_lambda: 50.0

+ xgboost_early_stopping_rounds: 20  # NEW

- rf_max_depth: 20
+ rf_max_depth: 10
```

#### 2. `src/utils/config.py`
- Ajout du paramÃ¨tre `xgboost_early_stopping_rounds` dans `ModelConfig`
- Mis Ã  jour dans `to_yaml()` pour sÃ©rialisation

#### 3. `src/models/tree_models.py`
- Ajout de `early_stopping_rounds` dans XGBRegressor
- Ajout de `eval_metric="mae"` pour early stopping

#### 4. `scripts/train.py`
- DÃ©tection spÃ©ciale pour XGBoost
- Split train/val (85/15) pour early stopping
- Passage de `eval_set` lors du fit
- Log de l'itÃ©ration optimale

---

## ğŸ¯ RÃ©sultats Attendus

### Ridge
- **Avant :** Overfitting 39.3x, MAE 3.415
- **AprÃ¨s (cible) :** Overfitting < 5x, MAE ~4.0-4.5
- **Compromis :** LÃ©gÃ¨re augmentation MAE pour Ã©liminer l'overfitting

### XGBoost
- **Avant :** Overfitting 63.1x, MAE 4.465
- **AprÃ¨s (cible) :** Overfitting < 5x, MAE ~5.0-6.0
- **BÃ©nÃ©fices :**
  - Early stopping = trouve itÃ©ration optimale
  - Moins d'arbres = entraÃ®nement 2x plus rapide
  - Meilleure gÃ©nÃ©ralisation

### ElasticNet & Lasso
- **Statut :** DÃ©jÃ  acceptables (< 10x)
- **Impact :** Minime, bÃ©nÃ©ficient de la rÃ©duction des features

### RandomForest
- **Avant :** Overfitting 2.6x âœ…
- **AprÃ¨s :** Devrait rester stable, peut-Ãªtre lÃ©gÃ¨rement meilleur

---

## ğŸ“Š Comment vÃ©rifier

```bash
# RÃ©-entraÃ®ner avec les nouveaux paramÃ¨tres
python scripts/train.py --config config/model_config.yaml

# VÃ©rifier les ratios d'overfitting dans les logs
grep "Overfitting:" | grep -E "(Ridge|XGBoost)"

# Objectif : Tous < 5.0x
```

**Logs Ã  surveiller :**
```
Ridge - Overfitting: X.XXx      # Doit Ãªtre < 5.0
XGBoost - Overfitting: X.XXx    # Doit Ãªtre < 5.0
Early stopping at iteration XX  # XGBoost doit s'arrÃªter tÃ´t
```

---

## ğŸ§ª Validation

### Test 1 : Overfitting Ratios
âœ… **PASS** si tous les modÃ¨les ont overfitting < 5x

### Test 2 : Performance Test Set
âœ… **PASS** si MAE test â‰¤ MAE train Ã— 5

### Test 3 : GÃ©nÃ©ralisation
âœ… **PASS** si RÂ² test > 0.85 (performances acceptables maintenues)

---

## ğŸ”¬ Principes AppliquÃ©s

### 1. **Bias-Variance Tradeoff**
- â†‘ RÃ©gularisation â†’ â†‘ Bias, â†“ Variance
- Accepter lÃ©gÃ¨re â†‘ bias (MAE train) pour â†“â†“ variance (overfitting)

### 2. **Occam's Razor**
- ModÃ¨les plus simples gÃ©nÃ©ralisent mieux
- â†“ Features, â†“ Depth, â†“ Estimators

### 3. **Early Stopping**
- ArrÃªt avant convergence complÃ¨te = rÃ©gularisation implicite
- BasÃ© sur validation = meilleure indication de gÃ©nÃ©ralisation

### 4. **Regularization**
- **L1 (Lasso, alpha)** : Sparsity, sÃ©lection features
- **L2 (Ridge, lambda)** : Shrinkage, petits coefficients
- **Elastic Net** : Combinaison L1+L2

---

## ğŸ“– RÃ©fÃ©rences

### Pourquoi l'overfitting est mauvais ?
- **En recherche :** RÃ©sultats non reproductibles
- **En production :** PrÃ©dictions catastrophiques sur nouvelles donnÃ©es
- **En clinique :** Diagnostics erronÃ©s, patients mal traitÃ©s

### Ratio acceptable
- **< 2x** : Excellent (RandomForest actuel : 2.6x)
- **< 5x** : Acceptable (Lasso actuel : 5.1x)
- **< 10x** : Limite (ElasticNet actuel : 8.2x)
- **> 10x** : âš ï¸ ProblÃ©matique (Ridge : 39x, XGBoost : 63x)

---

## âœ… Checklist

- [x] RÃ©duction features (10k â†’ 5k)
- [x] Augmentation Ridge alpha (100 â†’ 5000)
- [x] Augmentation XGBoost reg_alpha (1 â†’ 10)
- [x] Augmentation XGBoost reg_lambda (10 â†’ 50)
- [x] RÃ©duction XGBoost n_estimators (400 â†’ 200)
- [x] RÃ©duction XGBoost max_depth (6 â†’ 4)
- [x] RÃ©duction RF max_depth (20 â†’ 10)
- [x] ImplÃ©mentation early stopping XGBoost
- [x] Ajout config parameter xgboost_early_stopping_rounds
- [x] Modification train.py pour XGBoost validation split
- [x] Documentation complÃ¨te

---

## ğŸš€ Prochaines Ã‰tapes

1. **RÃ©-entraÃ®ner :**
   ```bash
   python scripts/train.py --config config/model_config.yaml
   ```

2. **VÃ©rifier logs :** Overfitting ratios < 5x ?

3. **Si toujours > 5x :**
   - Ridge : â†‘ alpha Ã  10000
   - XGBoost : â†‘ reg_lambda Ã  100

4. **Si < 5x mais MAE trop haute :**
   - Ã‰quilibre trouvÃ© ! âœ…
   - Trade-off acceptable

5. **Analyser avec l'app :**
   ```bash
   python app.py
   # VÃ©rifier graphiques de gÃ©nÃ©ralisation
   ```

---

**Date :** 2026-01-28
**Auteur :** Claude Opus 4.5
**Status :** âœ… Solutions implÃ©mentÃ©es, prÃªt pour rÃ©-entraÃ®nement
