# Hyperparameter Optimization - Guide D√©marrage Rapide ‚ö°

## üöÄ Lancement en 3 √âtapes

### 1Ô∏è‚É£ Installation (2 minutes)

```bash
# Installer les d√©pendances n√©cessaires
pip install optuna lightgbm catboost

# V√©rifier installation
python -c "import optuna, lightgbm, catboost; print('‚úì OK')"
```

### 2Ô∏è‚É£ Lancer l'Optimisation (6-8 heures)

#### Option Recommand√©e: Mode Standard

```bash
python scripts/hyperparameter_optimization.py
```

**Ce qui va se passer:**
- ‚úÖ Charge vos donn√©es (Data/)
- ‚úÖ S√©lectionne 5000 meilleurs sites CpG
- ‚úÖ Teste 10 algorithmes ML diff√©rents
- ‚úÖ 50-150 trials par algorithme
- ‚úÖ Sauvegarde progressive dans `results/optimization/`
- ‚úÖ Temps estim√©: 6-8 heures

**Surveiller la progression:**
```bash
# Dans un autre terminal
tail -f logs/*.log  # ou regarder la sortie console
```

#### Option Rapide: Mode PCA (4-6 heures)

```bash
python scripts/hyperparameter_optimization.py --use-pca --pca-components 400
```

Plus rapide, utilise moins de m√©moire!

### 3Ô∏è‚É£ Analyser les R√©sultats (1 minute)

```bash
python scripts/analyze_optimization.py
```

**G√©n√®re:**
- üìä Graphiques comparatifs
- üìã Tableau d√©taill√©
- üìà Analyse overfitting
- ‚è±Ô∏è Statistiques temps

---

## üìä Interpr√©ter les R√©sultats

### Fichier Principal: `optimization_results_*.csv`

```csv
Rank,Model,MAE_Test,R2_Test,Overfitting_Ratio,...
1,LightGBM,3.234,0.9678,1.51,...
2,XGBoost,3.298,0.9665,1.48,...
3,Ridge,3.412,0.9634,1.33,...
```

**Colonnes cl√©s:**
- **MAE_Test**: Erreur absolue moyenne (PLUS BAS = MEILLEUR) ‚≠ê
- **R2_Test**: Variance expliqu√©e (PLUS HAUT = MEILLEUR)
- **Overfitting_Ratio**: MAE_Test/MAE_Train (< 2.0 = excellent)

### Quel est le Meilleur Mod√®le?

**Cherchez:**
1. ‚úÖ **MAE_Test le plus bas** (rang 1)
2. ‚úÖ **Overfitting_Ratio < 2.0** (bon √©quilibre)
3. ‚úÖ **R2_Test > 0.95** (excellente pr√©cision)

**Exemple:**
```
Rank 1: LightGBM
  MAE Test: 3.234 ans  ‚Üê Erreur moyenne de pr√©diction
  R¬≤ Test: 0.9678      ‚Üê Explique 96.78% de la variance
  Overfitting: 1.51x   ‚Üê Excellente g√©n√©ralisation!
```

---

## üéØ Utiliser le Meilleur Mod√®le

### Charger et Pr√©dire

```python
import joblib
import pandas as pd
import numpy as np

# 1. Charger le meilleur mod√®le (exemple: LightGBM)
model = joblib.load('results/optimization/best_lightgbm.joblib')
scaler = joblib.load('results/optimization/scaler.joblib')
imputer = joblib.load('results/optimization/imputer.joblib')

# 2. Pr√©parer nouvelles donn√©es
X_new = pd.read_csv('mes_nouvelles_donnees.csv')  # Vos donn√©es
X_new = imputer.transform(X_new)
X_new = scaler.transform(X_new)

# 3. Pr√©dire
ages_predits = model.predict(X_new)
print(f"Ages pr√©dits: {ages_predits}")
```

---

## üí° Options Avanc√©es

### Optimiser Seulement Certains Mod√®les

```bash
# Seulement les 3 plus rapides
python scripts/hyperparameter_optimization.py --models Ridge Lasso ElasticNet --max-hours 2

# Seulement gradient boosting (les meilleurs g√©n√©ralement)
python scripts/hyperparameter_optimization.py --models XGBoost LightGBM CatBoost --max-hours 4
```

### Ajuster le Budget Temps

```bash
# Test rapide (1 heure)
python scripts/hyperparameter_optimization.py --max-hours 1 --top-k-features 1000

# Overnight (12 heures max)
python scripts/hyperparameter_optimization.py --max-hours 12
```

### Changer Nombre de Features

```bash
# Plus de features (meilleure pr√©cision, plus lent)
python scripts/hyperparameter_optimization.py --top-k-features 10000 --max-hours 10

# Moins de features (plus rapide)
python scripts/hyperparameter_optimization.py --top-k-features 2000 --max-hours 4
```

---

## ‚ùì FAQ

### Q: Combien de temps √ßa prend vraiment?

**R:** D√©pend de votre configuration:
- **Mode rapide** (PCA 400, 3 mod√®les): ~2h
- **Mode standard** (5000 features, tous mod√®les): ~6-8h
- **Mode complet** (10000 features, tous mod√®les): ~10-12h

### Q: Combien de RAM n√©cessaire?

**R:**
- **Mode PCA**: 4 GB minimum, 8 GB recommand√©
- **Mode standard (5000 features)**: 8 GB minimum, 16 GB recommand√©
- **Mode complet (10000+ features)**: 16 GB minimum, 32 GB recommand√©

### Q: Puis-je interrompre et reprendre?

**R:** Oui et non:
- ‚úÖ **Vous pouvez interrompre** (Ctrl+C) √† tout moment
- ‚úÖ **R√©sultats compl√©t√©s sont sauvegard√©s** (SQLite + joblib)
- ‚ùå **Impossible de reprendre exactement** o√π vous √©tiez
- ‚úÖ **Mais vous pouvez relancer** et ignorer mod√®les d√©j√† faits

### Q: Quel mod√®le choisir si plusieurs similaires?

**R:** Crit√®res de choix:
1. **Performance proche** ‚Üí Choisir le plus **simple** (Ridge > XGBoost en complexit√©)
2. **M√™me performance** ‚Üí Choisir le plus **rapide** (LightGBM > CatBoost)
3. **Production** ‚Üí Choisir le plus **stable** (Random Forest tr√®s stable)
4. **Interpr√©tabilit√©** ‚Üí Choisir **lin√©aire** (Ridge, Lasso, ElasticNet)

### Q: MAE de 3-4 ans, c'est bien?

**R:**
- **< 3 ans**: Excellent! √âtat de l'art
- **3-4 ans**: Tr√®s bon! Comparable √† Horvath/Hannum
- **4-5 ans**: Bon, acceptable
- **> 6 ans**: Probl√©matique, revoir approche

### Q: Overfitting ratio, qu'est-ce qui est bon?

**R:**
- **< 1.5x**: Excellent, parfait √©quilibre
- **1.5-2.5x**: Tr√®s bon, g√©n√©ralisation acceptable
- **2.5-5.0x**: Limite, surveiller
- **> 5.0x**: Probl√®me, overfitting s√©v√®re

---

## üîß Troubleshooting Rapide

### ‚ùå Erreur: "Out of Memory"

```bash
# Solution 1: Utiliser PCA
python scripts/hyperparameter_optimization.py --use-pca --pca-components 200

# Solution 2: Moins de features
python scripts/hyperparameter_optimization.py --top-k-features 1000
```

### ‚ùå Erreur: "Module 'lightgbm' not found"

```bash
pip install lightgbm catboost optuna
```

### ‚ö†Ô∏è Warning: "Trial pruned"

C'est normal! Optuna arr√™te les trials non prometteurs pour gagner du temps.

### üêå C'est trop lent!

```bash
# Option 1: Moins de mod√®les
python scripts/hyperparameter_optimization.py --models XGBoost LightGBM

# Option 2: PCA
python scripts/hyperparameter_optimization.py --use-pca --pca-components 200

# Option 3: Moins de features
python scripts/hyperparameter_optimization.py --top-k-features 2000
```

---

## üìà Exemple Complet de Bout en Bout

```bash
# Terminal 1: Lancer optimisation
python scripts/hyperparameter_optimization.py

# [Attendre 6-8 heures... ‚òï]

# Terminal 2: Analyser r√©sultats
python scripts/analyze_optimization.py

# Python: Utiliser meilleur mod√®le
python
>>> import joblib
>>> model = joblib.load('results/optimization/best_lightgbm.joblib')
>>> # Pr√©dire avec nouvelles donn√©es...
```

---

## üéì Pour Aller Plus Loin

**Lire la documentation compl√®te:**
```bash
cat HYPERPARAMETER_OPTIMIZATION.md  # Documentation d√©taill√©e
```

**Explorer base Optuna:**
```python
import optuna
storage = 'sqlite:///results/optimization/optuna_study.db'
studies = optuna.study.get_all_study_names(storage)
print(studies)  # Liste toutes les √©tudes
```

**Visualisations interactives Optuna:**
```python
import optuna
from optuna.visualization import plot_optimization_history

storage = 'sqlite:///results/optimization/optuna_study.db'
study = optuna.load_study(study_name='XGBoost_...', storage=storage)

# Historique optimisation
fig = plot_optimization_history(study)
fig.show()

# Importance param√®tres
fig = plot_param_importances(study)
fig.show()
```

---

**Bonne optimisation! üöÄ**

*Temps total lecture: 5 minutes*
*Temps total setup: 2 minutes*
*Temps total optimisation: 6-8 heures*
*Temps total analyse: 1 minute*

**Total: ~8 heures (dont 7h45 automatis√©) pour trouver le MEILLEUR mod√®le possible!**
